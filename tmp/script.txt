Hi, my name is Mitya, I’m from the Meta Site Republic group.
CLICK
Almost three years ago for Wix Turbo...
CLICK
… we created a fully async HTTP proxy that was deployed in small data centers across the world. Initially it served about 100K RPM.
 
Soon enough we started to handle entire User Of User traffic. And as proxy matured, it was decided to proxy entire external Wix traffic, and so...
CLICK
The quest for 3 million RPM has begun.
CLICK
According to wikipedia, resilience is the ability of the system to operate ok-ish under different conditions.

It’s a very important property of our proxies, because if proxy is down - Wix is down.

And the incidents are given to us to (figure out | realize | understand | discover) the variety of conditions!
CLICK
At some point we started to get OutOfMemoryError. Not often, not many, but it’s a sinister sign.
CLICK
Thanks to our excessive logging we managed to find the offending request
CLICK
And of course, it was somehow related to nodejs :)

Here you may see the request that was successfully completed in almost 2 minutes, streaming 3.3 gigabytes.
CLICK
The problem is simple - if the client is slower than a backend, then the proxy will be reading bytes from the backend into memory. In order to tackle this problem we just need to properly handle buffering: stop reading bytes from the backend when the buffer reaches a certain threshold. In netty it’s called watermarks and channel writability.

The takeaway from this incident is simple and directly related to resilience: systems should behave correctly even when we hit some unusual usage pattern, in this case - very large payloads and slow clients.
CLICK
The next epi… I mean incident was far more destructive.
CLICK
Requests flooded not pre-warmed up DC
(CLICK) In cold DC we have like 10 instances
(CLICK) We got peak time traffic on those 10 instances, where normally we would need one hundred.
(CLICK) With all these requests apps were choked up. And until we manually increased the number of instances it was recovering very slowly.

Obviously the issue is larger than just proxy application, but we can learn a few things from this incident.
CLICK
Backpressure is important. The application itself should somehow decide whether the request can be handled or not. Hopefully before the application goes insane.
This way it’s possible to make a fast fallback or at least show users “come back later” screen.

(CLICK) We haven’t implemented it yet, waiting for the next episode :)
CLICK
The way the incident was resolved is manual scale up. Why didn’t auto-scaling help in time?
CLICK
(CLICK) First thing we found out is that our startup time is very slow. Up to 3 minutes!
(CLICK) We use a KV Store that has a lot of data. And its initialization takes all this time. We tried to improve KV Store performance, but unsuccessfully.
(CLICK) So, we decided to proxy KV Store. We created a separate service that has KV Store in memory and exposes it via HTTP. This way it takes a few seconds to download and load into memory the entire KV Store. And then we just update it periodically in the background.
(CLICK) And now startup time is below 20 seconds. Faster startup time - faster auto-scaling.
CLICK
And the last part for today. NewRelic… Personally, I never liked it. But, from necessity a long time ago, with amazing advances in our monitoring systems, I started to think more and more about getting rid of NewRelic. (CLICK) And finally I may say (CLICK) Rest in Peace, NewRelic.
CLICK
But is it reasonable? I decided to investigate, so I did a heap dump of a proxy app and
(CLICK) found out that NewRelic takes about seventy five percent of all the memory! Let’s turn it off!
(CLICK) Back then our main metric about app health was GC Pause Time. After turning off the NewRelic GC Pause Time dropped from 500 millis to only 60!
(CLICK) Memory consumption dropped. Now we are able to handle more requests without sacrificing application health.
(CLICK) Happily, NewRelic operates more or less well with regular bootstrap and loom apps. So don’t expect anything big for yourself.
CLICK
Just to reiterate

(CLICK) Async is cool
(CLICK) Don’t let slow clients kill your app.
(CLICK) Backpressure is essential.
(CLICK) Esoteric solutions may be viable.
(CLICK) Our monitoring is amazing!
CLICK
And for Resiliency we have to do whatever it takes!
CLICK
THANK YOU
